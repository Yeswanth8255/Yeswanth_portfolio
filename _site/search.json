[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yeswanth Chitirala",
    "section": "",
    "text": "BIO\nThis is Yeswanth Chitirala. Currently, I am pursuing Masters in Advanced Data Analytics course. I have completed my under graduation in Mechanical engineering from RVR & JC college of engineering, Guntur, India. As I am very keen on physics and mathematics, I have chosen mechanical stream in my under graduation. After my undergrad I worked in Infosys for three years and worked in Deloitte for one year as ETL developer. I have 4 years of work experience in ETL informatica tool. My background for programming languages is I have worked up on C Programming, python and Oracle SQL. Coming to usage of analytics in my professional career, I used to clean the unprocessed data before transforming the data. As this course work is an extension to my work experience in ETL, I have chosen ADTA program."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Linear regression analysis",
    "section": "",
    "text": "This is a post with executable code.\n\n# Load required libraries\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(caret)\n\nLoading required package: lattice\n\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate a random dataframe\nn &lt;- 100\nrandom_df &lt;- data.frame(\n  X1 = rnorm(n),\n  X2 = rnorm(n),\n  X3 = rnorm(n)\n)\n\n# Add the Y column based on X1, X2, and X3\nrandom_df$Y &lt;- 2 * random_df$X1 - 3 * random_df$X2 + 0.5 * random_df$X3 + rnorm(n)\n\n# Display the first few rows of the dataframe\nprint(\"Random Dataframe:\")\n\n[1] \"Random Dataframe:\"\n\nprint(head(random_df))\n\n           X1          X2         X3          Y\n1 -0.56047565 -0.71040656  2.1988103  1.3944314\n2 -0.23017749  0.25688371  1.3124130 -1.3274886\n3  1.55870831 -0.24669188 -0.2651451  2.7863810\n4  0.07050839 -0.34754260  0.5431941  0.4027283\n5  0.12928774 -0.95161857 -0.4143399  2.4691017\n6  1.71506499 -0.04502772 -0.4762469  3.6582689"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am Yeswanth Chitirala, currently pursuing a Master’s in Advanced Data Analytics. My academic background includes a Bachelor’s degree in Mechanical Engineering from RVR & JC College of Engineering in Guntur, India. Despite my initial choice of the mechanical stream, I have a strong affinity for physics and mathematics.Following my undergraduate studies, I accumulated four years of work experience in the field of ETL (Extract, Transform, Load) development. I spent three years at Infosys and an additional year at Deloitte as an ETL developer, primarily using the Informatica tool. My programming skills encompass C, Python, and Oracle SQL. In my professional journey, I specialized in data analytics, focusing on cleaning unprocessed data before its transformation. This hands-on experience with ETL processes led me to pursue the Advanced Data Analytics program, where I aim to further enhance my skills and knowledge in this field."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Yeswanth Chitirala",
    "section": "",
    "text": "An enthusiastic and self-motivated Software Engineer with an extensive experience of 2 years in designing, developing, and testing in backend and front-end development. Strong background in building Applications using various technologies. Capable of working in a team environment or working independently. I am continually seeking new challenges and a desire to expand knowledge and experience."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "my blog page",
    "section": "",
    "text": "Exploring XGBoost: Predictive Modeling and Visualization\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJan 14, 2024\n\n\nYeswanth Chitirala\n\n\n\n\n\n\n  \n\n\n\n\nSupport Vector Machine (SVM) model\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJan 3, 2024\n\n\nYeswanth Chitirala\n\n\n\n\n\n\n  \n\n\n\n\nLinear regression analysis\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 14, 2023\n\n\nYeswanth Chitirala\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html#introduction",
    "href": "posts/post-with-code/index.html#introduction",
    "title": "Linear regression analysis",
    "section": "",
    "text": "This is a post with executable code.\n\n# Load required libraries\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(caret)\n\nLoading required package: lattice\n\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate a random dataframe\nn &lt;- 100\nrandom_df &lt;- data.frame(\n  X1 = rnorm(n),\n  X2 = rnorm(n),\n  X3 = rnorm(n)\n)\n\n# Add the Y column based on X1, X2, and X3\nrandom_df$Y &lt;- 2 * random_df$X1 - 3 * random_df$X2 + 0.5 * random_df$X3 + rnorm(n)\n\n# Display the first few rows of the dataframe\nprint(\"Random Dataframe:\")\n\n[1] \"Random Dataframe:\"\n\nprint(head(random_df))\n\n           X1          X2         X3          Y\n1 -0.56047565 -0.71040656  2.1988103  1.3944314\n2 -0.23017749  0.25688371  1.3124130 -1.3274886\n3  1.55870831 -0.24669188 -0.2651451  2.7863810\n4  0.07050839 -0.34754260  0.5431941  0.4027283\n5  0.12928774 -0.95161857 -0.4143399  2.4691017\n6  1.71506499 -0.04502772 -0.4762469  3.6582689"
  },
  {
    "objectID": "posts/post-with-code/index.html#linear-regression-analysis",
    "href": "posts/post-with-code/index.html#linear-regression-analysis",
    "title": "Linear regression analysis",
    "section": "Linear regression analysis",
    "text": "Linear regression analysis\n\n# Linear Regression Machine Learning\n# Split data into training and testing sets\nset.seed(456)\ntrain_index &lt;- createDataPartition(random_df$Y, p = 0.8, list = FALSE)\ntrain_data &lt;- random_df[train_index, ]\ntest_data &lt;- random_df[-train_index, ]\n\n# Fit a linear regression model\nlm_model &lt;- lm(Y ~ ., data = train_data)\n\n# Display the summary of the linear regression model\nprint(\"Linear Regression Model Summary:\")\n\n[1] \"Linear Regression Model Summary:\"\n\nprint(summary(lm_model))\n\n\nCall:\nlm(formula = Y ~ ., data = train_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.34439 -0.69371  0.03461  0.62794  2.66797 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.07543    0.12207  -0.618  0.53847    \nX1           2.07202    0.12921  16.036  &lt; 2e-16 ***\nX2          -2.87535    0.12951 -22.202  &lt; 2e-16 ***\nX3           0.36562    0.12312   2.970  0.00399 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.06 on 76 degrees of freedom\nMultiple R-squared:  0.9162,    Adjusted R-squared:  0.9129 \nF-statistic: 277.1 on 3 and 76 DF,  p-value: &lt; 2.2e-16\n\n# Make predictions on the test set\npredictions &lt;- predict(lm_model, newdata = test_data)\n\n# Evaluate the model\nprint(\"Model Evaluation:\")\n\n[1] \"Model Evaluation:\"\n\n# Calculate various forms of accuracy metrics\nrmse &lt;- sqrt(mean((predictions - test_data$Y)^2))\nmae &lt;- mean(abs(predictions - test_data$Y))\nr_squared &lt;- cor(predictions, test_data$Y)^2\n\nprint(paste(\"Root Mean Squared Error (RMSE):\", round(rmse, 3)))\n\n[1] \"Root Mean Squared Error (RMSE): 1.088\"\n\nprint(paste(\"Mean Absolute Error (MAE):\", round(mae, 3)))\n\n[1] \"Mean Absolute Error (MAE): 0.892\"\n\nprint(paste(\"R-squared:\", round(r_squared, 3)))\n\n[1] \"R-squared: 0.91\"\n\n# Additional Accuracy Metrics\nprint(\"Additional Accuracy Metrics:\")\n\n[1] \"Additional Accuracy Metrics:\"\n\n# Mean Squared Error (MSE)\nmse &lt;- mean((predictions - test_data$Y)^2)\n# Mean Absolute Percentage Error (MAPE)\nmape &lt;- mean(abs((test_data$Y - predictions) / test_data$Y)) * 100\n\nprint(paste(\"Mean Squared Error (MSE):\", round(mse, 3)))\n\n[1] \"Mean Squared Error (MSE): 1.184\"\n\nprint(paste(\"Mean Absolute Percentage Error (MAPE):\", round(mape, 3), \"%\"))\n\n[1] \"Mean Absolute Percentage Error (MAPE): 66.624 %\"\n\n\n\n# Interactive Visualization using plotly\nplot_ly(data = test_data, x = ~test_data$Y, y = ~predictions, type = 'scatter', mode = 'markers') %&gt;%\n  add_trace(x = c(min(test_data$Y), max(test_data$Y)), y = c(min(test_data$Y), max(test_data$Y)), mode = 'lines', line = list(color = 'red', dash = 'dash')) %&gt;%\n  layout(title = \"Interactive Predictions vs. Actual Values\", xaxis = list(title = \"Actual Values\"), yaxis = list(title = \"Predictions\"))\n\n\n\n\n# Residual Analysis\nprint(\"Residual Analysis:\")\n\n[1] \"Residual Analysis:\"\n\nresiduals &lt;- residuals(lm_model)\nqqnorm(residuals, main = \"Normal Q-Q Plot of Residuals\")\nqqline(residuals)\n\n\n\nplot(residuals, main = \"Residuals vs. Fitted Values\", xlab = \"Fitted Values\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\", lty = 2)\n\n\n\n# Visualize the predictions vs. actual values using ggplot2 (with residuals)\nresidual_data &lt;- data.frame(Actual = test_data$Y, Predicted = predictions, Residual = residuals)\n\nWarning in data.frame(Actual = test_data$Y, Predicted = predictions, Residual =\nresiduals): row names were found from a short variable and have been discarded\n\nggplot(residual_data) +\n  geom_point(aes(x = Actual, y = Predicted), color = \"blue\") +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Predictions vs. Actual Values with Residuals\", x = \"Actual Values\", y = \"Predictions\") +\n  geom_segment(aes(x = Actual, y = Predicted, xend = Actual, yend = Residual), color = \"gray\", alpha = 0.5) +\n  theme_minimal()\n\n\n\n# Interactive 3D Visualization using plotly\nprint(\"Interactive 3D Visualization:\")\n\n[1] \"Interactive 3D Visualization:\"\n\nplot_ly(\n  data = random_df,\n  x = ~X1, y = ~X2, z = ~X3,\n  type = 'scatter3d',\n  mode = 'markers',\n  marker = list(size = 5, color = 'blue')\n) %&gt;%\n  layout(title = \"Interactive 3D Scatterplot\", scene = list(xaxis = list(title = \"X1\"), yaxis = list(title = \"X2\"), zaxis = list(title = \"X3\")))\n\n\n\n\n# Pairwise correlation heatmap\nprint(\"Pairwise Correlation Heatmap:\")\n\n[1] \"Pairwise Correlation Heatmap:\"\n\nheatmap(cor_matrix, col = colorRampPalette(c(\"blue\", \"white\", \"red\"))(50), symm = TRUE)"
  },
  {
    "objectID": "posts/post-with-code/index.html#conclusion",
    "href": "posts/post-with-code/index.html#conclusion",
    "title": "Linear regression analysis",
    "section": "Conclusion",
    "text": "Conclusion\nThe linear regression model exhibits strong performance as indicated by various metrics. The model captures a substantial portion of the variance in the data, with an R-squared value of 0.91, suggesting that approximately 91% of the variability in the response variable is explained by the predictors. The coefficients of the predictors (X1, X2, and X3) are statistically significant, and their estimated values align with the expected relationships. In terms of accuracy metrics, the model demonstrates a Root Mean Squared Error (RMSE) of 1.088, implying that, on average, predictions deviate by approximately 1.088 units from the actual values. The Mean Absolute Error (MAE) is 0.892, representing the average absolute difference between predicted and actual values. Additionally, the Mean Squared Error (MSE) is 1.184, providing another perspective on the model’s predictive accuracy. It’s noteworthy that the Residual Standard Error is 1.06, and the F-statistic is 277.1 with a very low p-value, indicating the overall significance of the model. These collectively suggest that the model fits the data well, and its predictions align closely with the observed values."
  },
  {
    "objectID": "posts/modelling/index.html",
    "href": "posts/modelling/index.html",
    "title": "Support Vector Machine (SVM) model",
    "section": "",
    "text": "In this R code, we are working towards the future goal of building and assessing a machine learning model, specifically a Support Vector Machine (SVM). The dataset, generated randomly, includes predictor variables (X1, X2, and X3) and a response variable (Y). Our objective is to predict Y based on the given predictors using an SVM algorithm. We will proceed by splitting the dataset into training and testing sets, allocating 80% for model training and reserving 20% for future evaluation. In the next steps, we will train the SVM model on the training data, aiming to capture underlying patterns for accurate predictions. Subsequently, we will utilize the trained model to make predictions on the test set. For a comprehensive assessment, we plan to evaluate the model’s performance, calculating metrics such as accuracy and visualizing results through a confusion matrix and scatter plot. The intention is to gain insights into the model’s predictive capabilities and potential areas for improvement in future iterations.\n\n# Load required libraries\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(caret)\n\nLoading required package: lattice\n\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nlibrary(e1071) \n\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate a random dataframe\nn &lt;- 100\nrandom_df &lt;- data.frame(\n  X1 = rnorm(n),\n  X2 = rnorm(n),\n  X3 = rnorm(n)\n)\n\n# Add the Y column based on X1, X2, and X3\nrandom_df$Y &lt;- 2 * random_df$X1 - 3 * random_df$X2 + 0.5 * random_df$X3 + rnorm(n)\n\n# Display the first few rows of the dataframe\nprint(\"Random Dataframe:\")\n\n[1] \"Random Dataframe:\"\n\nprint(head(random_df))\n\n           X1          X2         X3          Y\n1 -0.56047565 -0.71040656  2.1988103  1.3944314\n2 -0.23017749  0.25688371  1.3124130 -1.3274886\n3  1.55870831 -0.24669188 -0.2651451  2.7863810\n4  0.07050839 -0.34754260  0.5431941  0.4027283\n5  0.12928774 -0.95161857 -0.4143399  2.4691017\n6  1.71506499 -0.04502772 -0.4762469  3.6582689"
  },
  {
    "objectID": "posts/modelling/index.html#introduction",
    "href": "posts/modelling/index.html#introduction",
    "title": "Support Vector Machine (SVM) model",
    "section": "",
    "text": "In this R code, we are working towards the future goal of building and assessing a machine learning model, specifically a Support Vector Machine (SVM). The dataset, generated randomly, includes predictor variables (X1, X2, and X3) and a response variable (Y). Our objective is to predict Y based on the given predictors using an SVM algorithm. We will proceed by splitting the dataset into training and testing sets, allocating 80% for model training and reserving 20% for future evaluation. In the next steps, we will train the SVM model on the training data, aiming to capture underlying patterns for accurate predictions. Subsequently, we will utilize the trained model to make predictions on the test set. For a comprehensive assessment, we plan to evaluate the model’s performance, calculating metrics such as accuracy and visualizing results through a confusion matrix and scatter plot. The intention is to gain insights into the model’s predictive capabilities and potential areas for improvement in future iterations.\n\n# Load required libraries\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(caret)\n\nLoading required package: lattice\n\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nlibrary(e1071) \n\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate a random dataframe\nn &lt;- 100\nrandom_df &lt;- data.frame(\n  X1 = rnorm(n),\n  X2 = rnorm(n),\n  X3 = rnorm(n)\n)\n\n# Add the Y column based on X1, X2, and X3\nrandom_df$Y &lt;- 2 * random_df$X1 - 3 * random_df$X2 + 0.5 * random_df$X3 + rnorm(n)\n\n# Display the first few rows of the dataframe\nprint(\"Random Dataframe:\")\n\n[1] \"Random Dataframe:\"\n\nprint(head(random_df))\n\n           X1          X2         X3          Y\n1 -0.56047565 -0.71040656  2.1988103  1.3944314\n2 -0.23017749  0.25688371  1.3124130 -1.3274886\n3  1.55870831 -0.24669188 -0.2651451  2.7863810\n4  0.07050839 -0.34754260  0.5431941  0.4027283\n5  0.12928774 -0.95161857 -0.4143399  2.4691017\n6  1.71506499 -0.04502772 -0.4762469  3.6582689"
  },
  {
    "objectID": "posts/modelling/index.html#modelling",
    "href": "posts/modelling/index.html#modelling",
    "title": "Support Vector Machine (SVM) model",
    "section": "Modelling",
    "text": "Modelling\n\n# Split the data into training and testing sets\ntrainIndex &lt;- createDataPartition(random_df$Y, p = 0.8, list = FALSE)\ntrain_data &lt;- random_df[trainIndex, ]\ntest_data &lt;- random_df[-trainIndex, ]\n\n# Train a Support Vector Machine (SVM) model\nsvm_model &lt;- svm(Y ~ ., data = train_data)\n\n# Make predictions on the test set\npredictions &lt;- predict(svm_model, newdata = test_data)\n\n# Evaluate the model\nconf_matrix &lt;- table(Actual = test_data$Y, Predicted = predictions)\naccuracy &lt;- sum(diag(conf_matrix)) / sum(conf_matrix)\n\n# Print detailed accuracy metrics\nprint(\"Confusion Matrix:\")\n\n[1] \"Confusion Matrix:\"\n\nprint(conf_matrix)\n\n                    Predicted\nActual               -5.65664381921355 -3.18108018354608 -3.16183221893515\n  -10.0854979973896                  0                 0                 0\n  -5.65490828406853                  1                 0                 0\n  -4.63790690416361                  0                 0                 1\n  -3.45919518577466                  0                 1                 0\n  -2.46051642793663                  0                 0                 0\n  -1.4855185126181                   0                 0                 0\n  -1.02943777752725                  0                 0                 0\n  -0.747187507913909                 0                 0                 0\n  -0.703916425855438                 0                 0                 0\n  -0.680630472595762                 0                 0                 0\n  1.774013335589                     0                 0                 0\n  2.39583698699594                   0                 0                 0\n  2.50335904663394                   0                 0                 0\n  2.57684106175132                   0                 0                 0\n  2.697448829756                     0                 0                 0\n  3.36063285521033                   0                 0                 0\n  3.65826887384452                   0                 0                 0\n  3.93357553214689                   0                 0                 0\n  5.01812398988059                   0                 0                 0\n  5.92422000001226                   0                 0                 0\n                    Predicted\nActual               -2.39426479954742 -2.02994865465174 -1.49207304124759\n  -10.0854979973896                  0                 0                 0\n  -5.65490828406853                  0                 0                 0\n  -4.63790690416361                  0                 0                 0\n  -3.45919518577466                  0                 0                 0\n  -2.46051642793663                  0                 1                 0\n  -1.4855185126181                   0                 0                 0\n  -1.02943777752725                  1                 0                 0\n  -0.747187507913909                 0                 0                 0\n  -0.703916425855438                 0                 0                 1\n  -0.680630472595762                 0                 0                 0\n  1.774013335589                     0                 0                 0\n  2.39583698699594                   0                 0                 0\n  2.50335904663394                   0                 0                 0\n  2.57684106175132                   0                 0                 0\n  2.697448829756                     0                 0                 0\n  3.36063285521033                   0                 0                 0\n  3.65826887384452                   0                 0                 0\n  3.93357553214689                   0                 0                 0\n  5.01812398988059                   0                 0                 0\n  5.92422000001226                   0                 0                 0\n                    Predicted\nActual               -0.774026837236475 -0.541667282156556 -0.397893190050167\n  -10.0854979973896                   0                  1                  0\n  -5.65490828406853                   0                  0                  0\n  -4.63790690416361                   0                  0                  0\n  -3.45919518577466                   0                  0                  0\n  -2.46051642793663                   0                  0                  0\n  -1.4855185126181                    0                  0                  1\n  -1.02943777752725                   0                  0                  0\n  -0.747187507913909                  1                  0                  0\n  -0.703916425855438                  0                  0                  0\n  -0.680630472595762                  0                  0                  0\n  1.774013335589                      0                  0                  0\n  2.39583698699594                    0                  0                  0\n  2.50335904663394                    0                  0                  0\n  2.57684106175132                    0                  0                  0\n  2.697448829756                      0                  0                  0\n  3.36063285521033                    0                  0                  0\n  3.65826887384452                    0                  0                  0\n  3.93357553214689                    0                  0                  0\n  5.01812398988059                    0                  0                  0\n  5.92422000001226                    0                  0                  0\n                    Predicted\nActual               -0.167020631615369 0.325630584431048 0.934884433347682\n  -10.0854979973896                   0                 0                 0\n  -5.65490828406853                   0                 0                 0\n  -4.63790690416361                   0                 0                 0\n  -3.45919518577466                   0                 0                 0\n  -2.46051642793663                   0                 0                 0\n  -1.4855185126181                    0                 0                 0\n  -1.02943777752725                   0                 0                 0\n  -0.747187507913909                  0                 0                 0\n  -0.703916425855438                  0                 0                 0\n  -0.680630472595762                  0                 0                 1\n  1.774013335589                      0                 0                 0\n  2.39583698699594                    0                 0                 0\n  2.50335904663394                    1                 0                 0\n  2.57684106175132                    0                 0                 0\n  2.697448829756                      0                 0                 0\n  3.36063285521033                    0                 1                 0\n  3.65826887384452                    0                 0                 0\n  3.93357553214689                    0                 0                 0\n  5.01812398988059                    0                 0                 0\n  5.92422000001226                    0                 0                 0\n                    Predicted\nActual               1.15376991391586 2.10950645904277 2.59208250219426\n  -10.0854979973896                 0                0                0\n  -5.65490828406853                 0                0                0\n  -4.63790690416361                 0                0                0\n  -3.45919518577466                 0                0                0\n  -2.46051642793663                 0                0                0\n  -1.4855185126181                  0                0                0\n  -1.02943777752725                 0                0                0\n  -0.747187507913909                0                0                0\n  -0.703916425855438                0                0                0\n  -0.680630472595762                0                0                0\n  1.774013335589                    0                0                0\n  2.39583698699594                  1                0                0\n  2.50335904663394                  0                0                0\n  2.57684106175132                  0                1                0\n  2.697448829756                    0                0                0\n  3.36063285521033                  0                0                0\n  3.65826887384452                  0                0                0\n  3.93357553214689                  0                0                1\n  5.01812398988059                  0                0                0\n  5.92422000001226                  0                0                0\n                    Predicted\nActual               3.12903569705979 3.24842717786761 3.61054373113523\n  -10.0854979973896                 0                0                0\n  -5.65490828406853                 0                0                0\n  -4.63790690416361                 0                0                0\n  -3.45919518577466                 0                0                0\n  -2.46051642793663                 0                0                0\n  -1.4855185126181                  0                0                0\n  -1.02943777752725                 0                0                0\n  -0.747187507913909                0                0                0\n  -0.703916425855438                0                0                0\n  -0.680630472595762                0                0                0\n  1.774013335589                    0                1                0\n  2.39583698699594                  0                0                0\n  2.50335904663394                  0                0                0\n  2.57684106175132                  0                0                0\n  2.697448829756                    0                0                0\n  3.36063285521033                  0                0                0\n  3.65826887384452                  1                0                0\n  3.93357553214689                  0                0                0\n  5.01812398988059                  0                0                1\n  5.92422000001226                  0                0                0\n                    Predicted\nActual               3.84661973754848 5.58376808649549\n  -10.0854979973896                 0                0\n  -5.65490828406853                 0                0\n  -4.63790690416361                 0                0\n  -3.45919518577466                 0                0\n  -2.46051642793663                 0                0\n  -1.4855185126181                  0                0\n  -1.02943777752725                 0                0\n  -0.747187507913909                0                0\n  -0.703916425855438                0                0\n  -0.680630472595762                0                0\n  1.774013335589                    0                0\n  2.39583698699594                  0                0\n  2.50335904663394                  0                0\n  2.57684106175132                  0                0\n  2.697448829756                    1                0\n  3.36063285521033                  0                0\n  3.65826887384452                  0                0\n  3.93357553214689                  0                0\n  5.01812398988059                  0                0\n  5.92422000001226                  0                1\n\nprint(paste(\"Accuracy:\", round(accuracy, 4)))\n\n[1] \"Accuracy: 0.2\"\n\n# Visualize the confusion matrix\nplot_conf_matrix &lt;- plot_ly(\n  x = rownames(conf_matrix),\n  y = colnames(conf_matrix),\n  z = as.matrix(conf_matrix),\n  type = \"heatmap\",\n  colorscale = \"Viridis\",\n  showscale = TRUE\n) %&gt;%\n  layout(title = \"Confusion Matrix\",\n         xaxis = list(title = \"Predicted\"),\n         yaxis = list(title = \"Actual\"))\n\n# Print the confusion matrix plot\nprint(plot_conf_matrix)"
  },
  {
    "objectID": "posts/random/index.html",
    "href": "posts/random/index.html",
    "title": "Exploring XGBoost: Predictive Modeling and Visualization",
    "section": "",
    "text": "# Load required libraries\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(caret)\n\nLoading required package: lattice\n\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nlibrary(xgboost)\n\n\nAttaching package: 'xgboost'\n\n\nThe following object is masked from 'package:plotly':\n\n    slice\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate a random dataframe\nn &lt;- 100\nrandom_df &lt;- data.frame(\n  X1 = rnorm(n),\n  X2 = rnorm(n),\n  X3 = rnorm(n)\n)\n\n# Add the Y column based on X1, X2, and X3\nrandom_df$Y &lt;- 2 * random_df$X1 - 3 * random_df$X2 + 0.5 * random_df$X3 + rnorm(n)\n\n# Display the first few rows of the dataframe\nprint(\"Random Dataframe:\")\n\n[1] \"Random Dataframe:\"\n\nprint(head(random_df))\n\n           X1          X2         X3          Y\n1 -0.56047565 -0.71040656  2.1988103  1.3944314\n2 -0.23017749  0.25688371  1.3124130 -1.3274886\n3  1.55870831 -0.24669188 -0.2651451  2.7863810\n4  0.07050839 -0.34754260  0.5431941  0.4027283\n5  0.12928774 -0.95161857 -0.4143399  2.4691017\n6  1.71506499 -0.04502772 -0.4762469  3.6582689"
  },
  {
    "objectID": "posts/random/index.html#introduction",
    "href": "posts/random/index.html#introduction",
    "title": "Exploring XGBoost: Predictive Modeling and Visualization",
    "section": "",
    "text": "# Load required libraries\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(caret)\n\nLoading required package: lattice\n\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nlibrary(xgboost)\n\n\nAttaching package: 'xgboost'\n\n\nThe following object is masked from 'package:plotly':\n\n    slice\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate a random dataframe\nn &lt;- 100\nrandom_df &lt;- data.frame(\n  X1 = rnorm(n),\n  X2 = rnorm(n),\n  X3 = rnorm(n)\n)\n\n# Add the Y column based on X1, X2, and X3\nrandom_df$Y &lt;- 2 * random_df$X1 - 3 * random_df$X2 + 0.5 * random_df$X3 + rnorm(n)\n\n# Display the first few rows of the dataframe\nprint(\"Random Dataframe:\")\n\n[1] \"Random Dataframe:\"\n\nprint(head(random_df))\n\n           X1          X2         X3          Y\n1 -0.56047565 -0.71040656  2.1988103  1.3944314\n2 -0.23017749  0.25688371  1.3124130 -1.3274886\n3  1.55870831 -0.24669188 -0.2651451  2.7863810\n4  0.07050839 -0.34754260  0.5431941  0.4027283\n5  0.12928774 -0.95161857 -0.4143399  2.4691017\n6  1.71506499 -0.04502772 -0.4762469  3.6582689"
  },
  {
    "objectID": "posts/random/index.html#modelling",
    "href": "posts/random/index.html#modelling",
    "title": "Exploring XGBoost: Predictive Modeling and Visualization",
    "section": "MODELLING",
    "text": "MODELLING\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Split the data into training and testing sets\ntrain_indices &lt;- createDataPartition(random_df$Y, p = 0.8, list = FALSE)\ntrain_data &lt;- random_df[train_indices, ]\ntest_data &lt;- random_df[-train_indices, ]\n\n# Create an XGBoost model\nxgb_model &lt;- xgboost(data = as.matrix(train_data[, -4]), label = train_data$Y, nrounds = 100)\n\n[1] train-rmse:2.726252 \n[2] train-rmse:2.107350 \n[3] train-rmse:1.645595 \n[4] train-rmse:1.302361 \n[5] train-rmse:1.047537 \n[6] train-rmse:0.858126 \n[7] train-rmse:0.716289 \n[8] train-rmse:0.602406 \n[9] train-rmse:0.500463 \n[10]    train-rmse:0.425746 \n[11]    train-rmse:0.367619 \n[12]    train-rmse:0.321461 \n[13]    train-rmse:0.273372 \n[14]    train-rmse:0.239109 \n[15]    train-rmse:0.215893 \n[16]    train-rmse:0.192315 \n[17]    train-rmse:0.178456 \n[18]    train-rmse:0.159898 \n[19]    train-rmse:0.146353 \n[20]    train-rmse:0.129411 \n[21]    train-rmse:0.121509 \n[22]    train-rmse:0.108144 \n[23]    train-rmse:0.101073 \n[24]    train-rmse:0.091165 \n[25]    train-rmse:0.081990 \n[26]    train-rmse:0.074913 \n[27]    train-rmse:0.070503 \n[28]    train-rmse:0.065063 \n[29]    train-rmse:0.059579 \n[30]    train-rmse:0.056038 \n[31]    train-rmse:0.053991 \n[32]    train-rmse:0.046893 \n[33]    train-rmse:0.043252 \n[34]    train-rmse:0.041789 \n[35]    train-rmse:0.039587 \n[36]    train-rmse:0.038043 \n[37]    train-rmse:0.032243 \n[38]    train-rmse:0.029073 \n[39]    train-rmse:0.025471 \n[40]    train-rmse:0.021839 \n[41]    train-rmse:0.018946 \n[42]    train-rmse:0.016405 \n[43]    train-rmse:0.013649 \n[44]    train-rmse:0.012050 \n[45]    train-rmse:0.010856 \n[46]    train-rmse:0.009680 \n[47]    train-rmse:0.008477 \n[48]    train-rmse:0.008045 \n[49]    train-rmse:0.007340 \n[50]    train-rmse:0.006393 \n[51]    train-rmse:0.006123 \n[52]    train-rmse:0.005258 \n[53]    train-rmse:0.004784 \n[54]    train-rmse:0.004194 \n[55]    train-rmse:0.003924 \n[56]    train-rmse:0.003618 \n[57]    train-rmse:0.003388 \n[58]    train-rmse:0.003133 \n[59]    train-rmse:0.002965 \n[60]    train-rmse:0.002738 \n[61]    train-rmse:0.002401 \n[62]    train-rmse:0.002220 \n[63]    train-rmse:0.002052 \n[64]    train-rmse:0.001931 \n[65]    train-rmse:0.001737 \n[66]    train-rmse:0.001617 \n[67]    train-rmse:0.001461 \n[68]    train-rmse:0.001391 \n[69]    train-rmse:0.001295 \n[70]    train-rmse:0.001220 \n[71]    train-rmse:0.001220 \n[72]    train-rmse:0.001220 \n[73]    train-rmse:0.001220 \n[74]    train-rmse:0.001220 \n[75]    train-rmse:0.001220 \n[76]    train-rmse:0.001220 \n[77]    train-rmse:0.001220 \n[78]    train-rmse:0.001220 \n[79]    train-rmse:0.001220 \n[80]    train-rmse:0.001220 \n[81]    train-rmse:0.001220 \n[82]    train-rmse:0.001220 \n[83]    train-rmse:0.001220 \n[84]    train-rmse:0.001220 \n[85]    train-rmse:0.001220 \n[86]    train-rmse:0.001220 \n[87]    train-rmse:0.001220 \n[88]    train-rmse:0.001220 \n[89]    train-rmse:0.001220 \n[90]    train-rmse:0.001220 \n[91]    train-rmse:0.001220 \n[92]    train-rmse:0.001220 \n[93]    train-rmse:0.001220 \n[94]    train-rmse:0.001220 \n[95]    train-rmse:0.001220 \n[96]    train-rmse:0.001220 \n[97]    train-rmse:0.001220 \n[98]    train-rmse:0.001220 \n[99]    train-rmse:0.001220 \n[100]   train-rmse:0.001220 \n\n# Make predictions on the test set\npredictions &lt;- predict(xgb_model, as.matrix(test_data[, -4]))\n\n# Evaluate model performance\naccuracy &lt;- sqrt(mean((predictions - test_data$Y)^2))\n\n# Display the accuracy\nprint(paste(\"Model RMSE:\", round(accuracy, 4)))\n\n[1] \"Model RMSE: 1.6629\""
  }
]