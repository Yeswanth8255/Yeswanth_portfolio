{
  "hash": "6ddea5ce726669867e31886133f918d7",
  "result": {
    "markdown": "---\ntitle: \"Exploring XGBoost: Predictive Modeling and Visualization\"\nauthor: \"Yeswanth Chitirala\"\ndate: \"2024-01-14\"\ncategories: [news, code, analysis]\nimage: \"image.jpg\"\n---\n\n\n## Introduction\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load required libraries\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(caret)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: lattice\n```\n:::\n\n```{.r .cell-code}\nlibrary(plotly)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'plotly'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:stats':\n\n    filter\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:graphics':\n\n    layout\n```\n:::\n\n```{.r .cell-code}\nlibrary(xgboost)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'xgboost'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:plotly':\n\n    slice\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:dplyr':\n\n    slice\n```\n:::\n\n```{.r .cell-code}\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate a random dataframe\nn <- 100\nrandom_df <- data.frame(\n  X1 = rnorm(n),\n  X2 = rnorm(n),\n  X3 = rnorm(n)\n)\n\n# Add the Y column based on X1, X2, and X3\nrandom_df$Y <- 2 * random_df$X1 - 3 * random_df$X2 + 0.5 * random_df$X3 + rnorm(n)\n\n# Display the first few rows of the dataframe\nprint(\"Random Dataframe:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Random Dataframe:\"\n```\n:::\n\n```{.r .cell-code}\nprint(head(random_df))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           X1          X2         X3          Y\n1 -0.56047565 -0.71040656  2.1988103  1.3944314\n2 -0.23017749  0.25688371  1.3124130 -1.3274886\n3  1.55870831 -0.24669188 -0.2651451  2.7863810\n4  0.07050839 -0.34754260  0.5431941  0.4027283\n5  0.12928774 -0.95161857 -0.4143399  2.4691017\n6  1.71506499 -0.04502772 -0.4762469  3.6582689\n```\n:::\n:::\n\n\n## MODELLING \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set seed for reproducibility\nset.seed(123)\n\n# Split the data into training and testing sets\ntrain_indices <- createDataPartition(random_df$Y, p = 0.8, list = FALSE)\ntrain_data <- random_df[train_indices, ]\ntest_data <- random_df[-train_indices, ]\n\n# Create an XGBoost model\nxgb_model <- xgboost(data = as.matrix(train_data[, -4]), label = train_data$Y, nrounds = 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]\ttrain-rmse:2.726252 \n[2]\ttrain-rmse:2.107350 \n[3]\ttrain-rmse:1.645595 \n[4]\ttrain-rmse:1.302361 \n[5]\ttrain-rmse:1.047537 \n[6]\ttrain-rmse:0.858126 \n[7]\ttrain-rmse:0.716289 \n[8]\ttrain-rmse:0.602406 \n[9]\ttrain-rmse:0.500463 \n[10]\ttrain-rmse:0.425746 \n[11]\ttrain-rmse:0.367619 \n[12]\ttrain-rmse:0.321461 \n[13]\ttrain-rmse:0.273372 \n[14]\ttrain-rmse:0.239109 \n[15]\ttrain-rmse:0.215893 \n[16]\ttrain-rmse:0.192315 \n[17]\ttrain-rmse:0.178456 \n[18]\ttrain-rmse:0.159898 \n[19]\ttrain-rmse:0.146353 \n[20]\ttrain-rmse:0.129411 \n[21]\ttrain-rmse:0.121509 \n[22]\ttrain-rmse:0.108144 \n[23]\ttrain-rmse:0.101073 \n[24]\ttrain-rmse:0.091165 \n[25]\ttrain-rmse:0.081990 \n[26]\ttrain-rmse:0.074913 \n[27]\ttrain-rmse:0.070503 \n[28]\ttrain-rmse:0.065063 \n[29]\ttrain-rmse:0.059579 \n[30]\ttrain-rmse:0.056038 \n[31]\ttrain-rmse:0.053991 \n[32]\ttrain-rmse:0.046893 \n[33]\ttrain-rmse:0.043252 \n[34]\ttrain-rmse:0.041789 \n[35]\ttrain-rmse:0.039587 \n[36]\ttrain-rmse:0.038043 \n[37]\ttrain-rmse:0.032243 \n[38]\ttrain-rmse:0.029073 \n[39]\ttrain-rmse:0.025471 \n[40]\ttrain-rmse:0.021839 \n[41]\ttrain-rmse:0.018946 \n[42]\ttrain-rmse:0.016405 \n[43]\ttrain-rmse:0.013649 \n[44]\ttrain-rmse:0.012050 \n[45]\ttrain-rmse:0.010856 \n[46]\ttrain-rmse:0.009680 \n[47]\ttrain-rmse:0.008477 \n[48]\ttrain-rmse:0.008045 \n[49]\ttrain-rmse:0.007340 \n[50]\ttrain-rmse:0.006393 \n[51]\ttrain-rmse:0.006123 \n[52]\ttrain-rmse:0.005258 \n[53]\ttrain-rmse:0.004784 \n[54]\ttrain-rmse:0.004194 \n[55]\ttrain-rmse:0.003924 \n[56]\ttrain-rmse:0.003618 \n[57]\ttrain-rmse:0.003388 \n[58]\ttrain-rmse:0.003133 \n[59]\ttrain-rmse:0.002965 \n[60]\ttrain-rmse:0.002738 \n[61]\ttrain-rmse:0.002401 \n[62]\ttrain-rmse:0.002220 \n[63]\ttrain-rmse:0.002052 \n[64]\ttrain-rmse:0.001931 \n[65]\ttrain-rmse:0.001737 \n[66]\ttrain-rmse:0.001617 \n[67]\ttrain-rmse:0.001461 \n[68]\ttrain-rmse:0.001391 \n[69]\ttrain-rmse:0.001295 \n[70]\ttrain-rmse:0.001220 \n[71]\ttrain-rmse:0.001220 \n[72]\ttrain-rmse:0.001220 \n[73]\ttrain-rmse:0.001220 \n[74]\ttrain-rmse:0.001220 \n[75]\ttrain-rmse:0.001220 \n[76]\ttrain-rmse:0.001220 \n[77]\ttrain-rmse:0.001220 \n[78]\ttrain-rmse:0.001220 \n[79]\ttrain-rmse:0.001220 \n[80]\ttrain-rmse:0.001220 \n[81]\ttrain-rmse:0.001220 \n[82]\ttrain-rmse:0.001220 \n[83]\ttrain-rmse:0.001220 \n[84]\ttrain-rmse:0.001220 \n[85]\ttrain-rmse:0.001220 \n[86]\ttrain-rmse:0.001220 \n[87]\ttrain-rmse:0.001220 \n[88]\ttrain-rmse:0.001220 \n[89]\ttrain-rmse:0.001220 \n[90]\ttrain-rmse:0.001220 \n[91]\ttrain-rmse:0.001220 \n[92]\ttrain-rmse:0.001220 \n[93]\ttrain-rmse:0.001220 \n[94]\ttrain-rmse:0.001220 \n[95]\ttrain-rmse:0.001220 \n[96]\ttrain-rmse:0.001220 \n[97]\ttrain-rmse:0.001220 \n[98]\ttrain-rmse:0.001220 \n[99]\ttrain-rmse:0.001220 \n[100]\ttrain-rmse:0.001220 \n```\n:::\n\n```{.r .cell-code}\n# Make predictions on the test set\npredictions <- predict(xgb_model, as.matrix(test_data[, -4]))\n\n# Evaluate model performance\naccuracy <- sqrt(mean((predictions - test_data$Y)^2))\n\n# Display the accuracy\nprint(paste(\"Model RMSE:\", round(accuracy, 4)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Model RMSE: 1.6629\"\n```\n:::\n:::\n\n\n# VISUALIZATION \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualize actual vs. predicted values using plotly\nplot_data <- data.frame(Actual = test_data$Y, Predicted = predictions)\n\nplot <- plot_ly(plot_data, x = ~Actual, y = ~Predicted, type = 'scatter', mode = 'markers',\n                marker = list(color = 'blue', size = 8),\n                text = paste(\"Actual: \", test_data$Y, \"<br>Predicted: \", round(predictions, 4)))\n\nlayout <- list(title = \"Actual vs. Predicted Values\",\n               xaxis = list(title = \"Actual Values\"),\n               yaxis = list(title = \"Predicted Values\"))\n\nplot <- plot %>% layout(layout)\n\n# Display the interactive plot\nprint(plot)\n```\n:::\n\n\nThe root mean squared error (RMSE) of the XGBoost model is 1.6629. RMSE is a measure of the average deviation between the predicted and actual values, and in this context, a lower RMSE indicates better model performance. In other words, the model, on average, is making predictions that are approximately 1.6629 units away from the true values. While assessing model accuracy, it's crucial to consider the scale and context of the problem at hand. Comparing the RMSE to the range of the target variable can provide insights into the practical significance of the error. In this case, the model's performance appears reasonable, but further analysis and comparison with alternative models or baseline approaches may provide a more comprehensive understanding of its effectiveness.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}