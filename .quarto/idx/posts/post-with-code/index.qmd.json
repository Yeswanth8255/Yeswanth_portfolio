{"title":"Linear regression analysis","markdown":{"yaml":{"title":"Linear regression analysis","author":"Yeswanth Chitirala","date":"2023-12-14","categories":["news","code","analysis"],"image":"image.jpg"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\nThis is a well curated data analysis using R and the linear regression machine learning model to predict factors affecting one variable from another and model accuracy in terms of that accuracy.\n\n```{r}\n# Load required libraries\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(plotly)\n\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate a random dataframe\nn <- 100\nrandom_df <- data.frame(\n  X1 = rnorm(n),\n  X2 = rnorm(n),\n  X3 = rnorm(n)\n)\n\n# Add the Y column based on X1, X2, and X3\nrandom_df$Y <- 2 * random_df$X1 - 3 * random_df$X2 + 0.5 * random_df$X3 + rnorm(n)\n\n# Display the first few rows of the dataframe\nprint(\"Random Dataframe:\")\nprint(head(random_df))\n\n\n```\n\n# Descriptive statistics\n\n```{r}\n# Perform Descriptive Statistics\nprint(\"Descriptive Statistics:\")\nprint(summary(random_df))\n\n```\n\n# Data visualization\n\n```{r}\n# Exploratory Data Analysis (EDA)\n# Scatterplot matrix\nprint(\"Scatterplot Matrix:\")\npairs(random_df)\n\n# Correlation matrix\nprint(\"Correlation Matrix:\")\ncor_matrix <- cor(random_df)\nprint(cor_matrix)\n\n# 3D Scatterplot\nprint(\"3D Scatterplot:\")\nscatterplot3d::scatterplot3d(\n  random_df$X1, random_df$X2, random_df$X3,\n  color = \"blue\",\n  main = \"3D Scatterplot\"\n)\n\n# Density Plots for each variable\nprint(\"Density Plots:\")\npar(mfrow = c(2, 2))\nfor (i in 1:3) {\n  density_plot <- ggplot(random_df, aes(x = random_df[, i])) +\n    geom_density(fill = \"blue\", color = \"black\") +\n    labs(title = paste(\"Density Plot for\", names(random_df)[i]))\n  print(density_plot)\n}\npar(mfrow = c(1, 1)) \n```\n\n## Linear regression analysis\n\n```{r}\n# Linear Regression Machine Learning\n# Split data into training and testing sets\nset.seed(456)\ntrain_index <- createDataPartition(random_df$Y, p = 0.8, list = FALSE)\ntrain_data <- random_df[train_index, ]\ntest_data <- random_df[-train_index, ]\n\n# Fit a linear regression model\nlm_model <- lm(Y ~ ., data = train_data)\n\n# Display the summary of the linear regression model\nprint(\"Linear Regression Model Summary:\")\nprint(summary(lm_model))\n\n# Make predictions on the test set\npredictions <- predict(lm_model, newdata = test_data)\n\n# Evaluate the model\nprint(\"Model Evaluation:\")\n# Calculate various forms of accuracy metrics\nrmse <- sqrt(mean((predictions - test_data$Y)^2))\nmae <- mean(abs(predictions - test_data$Y))\nr_squared <- cor(predictions, test_data$Y)^2\n\nprint(paste(\"Root Mean Squared Error (RMSE):\", round(rmse, 3)))\nprint(paste(\"Mean Absolute Error (MAE):\", round(mae, 3)))\nprint(paste(\"R-squared:\", round(r_squared, 3)))\n# Additional Accuracy Metrics\nprint(\"Additional Accuracy Metrics:\")\n# Mean Squared Error (MSE)\nmse <- mean((predictions - test_data$Y)^2)\n# Mean Absolute Percentage Error (MAPE)\nmape <- mean(abs((test_data$Y - predictions) / test_data$Y)) * 100\n\nprint(paste(\"Mean Squared Error (MSE):\", round(mse, 3)))\nprint(paste(\"Mean Absolute Percentage Error (MAPE):\", round(mape, 3), \"%\"))\n\n```\n\n```{r}\n# Interactive Visualization using plotly\nplot_ly(data = test_data, x = ~test_data$Y, y = ~predictions, type = 'scatter', mode = 'markers') %>%\n  add_trace(x = c(min(test_data$Y), max(test_data$Y)), y = c(min(test_data$Y), max(test_data$Y)), mode = 'lines', line = list(color = 'red', dash = 'dash')) %>%\n  layout(title = \"Interactive Predictions vs. Actual Values\", xaxis = list(title = \"Actual Values\"), yaxis = list(title = \"Predictions\"))\n\n# Residual Analysis\nprint(\"Residual Analysis:\")\nresiduals <- residuals(lm_model)\nqqnorm(residuals, main = \"Normal Q-Q Plot of Residuals\")\nqqline(residuals)\nplot(residuals, main = \"Residuals vs. Fitted Values\", xlab = \"Fitted Values\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\", lty = 2)\n\n# Visualize the predictions vs. actual values using ggplot2 (with residuals)\nresidual_data <- data.frame(Actual = test_data$Y, Predicted = predictions, Residual = residuals)\n\nggplot(residual_data) +\n  geom_point(aes(x = Actual, y = Predicted), color = \"blue\") +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Predictions vs. Actual Values with Residuals\", x = \"Actual Values\", y = \"Predictions\") +\n  geom_segment(aes(x = Actual, y = Predicted, xend = Actual, yend = Residual), color = \"gray\", alpha = 0.5) +\n  theme_minimal()\n\n# Interactive 3D Visualization using plotly\nprint(\"Interactive 3D Visualization:\")\nplot_ly(\n  data = random_df,\n  x = ~X1, y = ~X2, z = ~X3,\n  type = 'scatter3d',\n  mode = 'markers',\n  marker = list(size = 5, color = 'blue')\n) %>%\n  layout(title = \"Interactive 3D Scatterplot\", scene = list(xaxis = list(title = \"X1\"), yaxis = list(title = \"X2\"), zaxis = list(title = \"X3\")))\n# Pairwise correlation heatmap\nprint(\"Pairwise Correlation Heatmap:\")\nheatmap(cor_matrix, col = colorRampPalette(c(\"blue\", \"white\", \"red\"))(50), symm = TRUE)\n```\n\n## Conclusion\n\nThe linear regression model exhibits strong performance as indicated by various metrics. The model captures a substantial portion of the variance in the data, with an R-squared value of 0.91, suggesting that approximately 91% of the variability in the response variable is explained by the predictors. The coefficients of the predictors (X1, X2, and X3) are statistically significant, and their estimated values align with the expected relationships. In terms of accuracy metrics, the model demonstrates a Root Mean Squared Error (RMSE) of 1.088, implying that, on average, predictions deviate by approximately 1.088 units from the actual values. The Mean Absolute Error (MAE) is 0.892, representing the average absolute difference between predicted and actual values. Additionally, the Mean Squared Error (MSE) is 1.184, providing another perspective on the model's predictive accuracy. It's noteworthy that the Residual Standard Error is 1.06, and the F-statistic is 277.1 with a very low p-value, indicating the overall significance of the model. These collectively suggest that the model fits the data well, and its predictions align closely with the observed values.\n","srcMarkdownNoYaml":"\n\n## Introduction\n\nThis is a well curated data analysis using R and the linear regression machine learning model to predict factors affecting one variable from another and model accuracy in terms of that accuracy.\n\n```{r}\n# Load required libraries\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(plotly)\n\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate a random dataframe\nn <- 100\nrandom_df <- data.frame(\n  X1 = rnorm(n),\n  X2 = rnorm(n),\n  X3 = rnorm(n)\n)\n\n# Add the Y column based on X1, X2, and X3\nrandom_df$Y <- 2 * random_df$X1 - 3 * random_df$X2 + 0.5 * random_df$X3 + rnorm(n)\n\n# Display the first few rows of the dataframe\nprint(\"Random Dataframe:\")\nprint(head(random_df))\n\n\n```\n\n# Descriptive statistics\n\n```{r}\n# Perform Descriptive Statistics\nprint(\"Descriptive Statistics:\")\nprint(summary(random_df))\n\n```\n\n# Data visualization\n\n```{r}\n# Exploratory Data Analysis (EDA)\n# Scatterplot matrix\nprint(\"Scatterplot Matrix:\")\npairs(random_df)\n\n# Correlation matrix\nprint(\"Correlation Matrix:\")\ncor_matrix <- cor(random_df)\nprint(cor_matrix)\n\n# 3D Scatterplot\nprint(\"3D Scatterplot:\")\nscatterplot3d::scatterplot3d(\n  random_df$X1, random_df$X2, random_df$X3,\n  color = \"blue\",\n  main = \"3D Scatterplot\"\n)\n\n# Density Plots for each variable\nprint(\"Density Plots:\")\npar(mfrow = c(2, 2))\nfor (i in 1:3) {\n  density_plot <- ggplot(random_df, aes(x = random_df[, i])) +\n    geom_density(fill = \"blue\", color = \"black\") +\n    labs(title = paste(\"Density Plot for\", names(random_df)[i]))\n  print(density_plot)\n}\npar(mfrow = c(1, 1)) \n```\n\n## Linear regression analysis\n\n```{r}\n# Linear Regression Machine Learning\n# Split data into training and testing sets\nset.seed(456)\ntrain_index <- createDataPartition(random_df$Y, p = 0.8, list = FALSE)\ntrain_data <- random_df[train_index, ]\ntest_data <- random_df[-train_index, ]\n\n# Fit a linear regression model\nlm_model <- lm(Y ~ ., data = train_data)\n\n# Display the summary of the linear regression model\nprint(\"Linear Regression Model Summary:\")\nprint(summary(lm_model))\n\n# Make predictions on the test set\npredictions <- predict(lm_model, newdata = test_data)\n\n# Evaluate the model\nprint(\"Model Evaluation:\")\n# Calculate various forms of accuracy metrics\nrmse <- sqrt(mean((predictions - test_data$Y)^2))\nmae <- mean(abs(predictions - test_data$Y))\nr_squared <- cor(predictions, test_data$Y)^2\n\nprint(paste(\"Root Mean Squared Error (RMSE):\", round(rmse, 3)))\nprint(paste(\"Mean Absolute Error (MAE):\", round(mae, 3)))\nprint(paste(\"R-squared:\", round(r_squared, 3)))\n# Additional Accuracy Metrics\nprint(\"Additional Accuracy Metrics:\")\n# Mean Squared Error (MSE)\nmse <- mean((predictions - test_data$Y)^2)\n# Mean Absolute Percentage Error (MAPE)\nmape <- mean(abs((test_data$Y - predictions) / test_data$Y)) * 100\n\nprint(paste(\"Mean Squared Error (MSE):\", round(mse, 3)))\nprint(paste(\"Mean Absolute Percentage Error (MAPE):\", round(mape, 3), \"%\"))\n\n```\n\n```{r}\n# Interactive Visualization using plotly\nplot_ly(data = test_data, x = ~test_data$Y, y = ~predictions, type = 'scatter', mode = 'markers') %>%\n  add_trace(x = c(min(test_data$Y), max(test_data$Y)), y = c(min(test_data$Y), max(test_data$Y)), mode = 'lines', line = list(color = 'red', dash = 'dash')) %>%\n  layout(title = \"Interactive Predictions vs. Actual Values\", xaxis = list(title = \"Actual Values\"), yaxis = list(title = \"Predictions\"))\n\n# Residual Analysis\nprint(\"Residual Analysis:\")\nresiduals <- residuals(lm_model)\nqqnorm(residuals, main = \"Normal Q-Q Plot of Residuals\")\nqqline(residuals)\nplot(residuals, main = \"Residuals vs. Fitted Values\", xlab = \"Fitted Values\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\", lty = 2)\n\n# Visualize the predictions vs. actual values using ggplot2 (with residuals)\nresidual_data <- data.frame(Actual = test_data$Y, Predicted = predictions, Residual = residuals)\n\nggplot(residual_data) +\n  geom_point(aes(x = Actual, y = Predicted), color = \"blue\") +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Predictions vs. Actual Values with Residuals\", x = \"Actual Values\", y = \"Predictions\") +\n  geom_segment(aes(x = Actual, y = Predicted, xend = Actual, yend = Residual), color = \"gray\", alpha = 0.5) +\n  theme_minimal()\n\n# Interactive 3D Visualization using plotly\nprint(\"Interactive 3D Visualization:\")\nplot_ly(\n  data = random_df,\n  x = ~X1, y = ~X2, z = ~X3,\n  type = 'scatter3d',\n  mode = 'markers',\n  marker = list(size = 5, color = 'blue')\n) %>%\n  layout(title = \"Interactive 3D Scatterplot\", scene = list(xaxis = list(title = \"X1\"), yaxis = list(title = \"X2\"), zaxis = list(title = \"X3\")))\n# Pairwise correlation heatmap\nprint(\"Pairwise Correlation Heatmap:\")\nheatmap(cor_matrix, col = colorRampPalette(c(\"blue\", \"white\", \"red\"))(50), symm = TRUE)\n```\n\n## Conclusion\n\nThe linear regression model exhibits strong performance as indicated by various metrics. The model captures a substantial portion of the variance in the data, with an R-squared value of 0.91, suggesting that approximately 91% of the variability in the response variable is explained by the predictors. The coefficients of the predictors (X1, X2, and X3) are statistically significant, and their estimated values align with the expected relationships. In terms of accuracy metrics, the model demonstrates a Root Mean Squared Error (RMSE) of 1.088, implying that, on average, predictions deviate by approximately 1.088 units from the actual values. The Mean Absolute Error (MAE) is 0.892, representing the average absolute difference between predicted and actual values. Additionally, the Mean Squared Error (MSE) is 1.184, providing another perspective on the model's predictive accuracy. It's noteworthy that the Residual Standard Error is 1.06, and the F-statistic is 277.1 with a very low p-value, indicating the overall significance of the model. These collectively suggest that the model fits the data well, and its predictions align closely with the observed values.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.353","editor":"visual","theme":"vapor","title-block-banner":true,"title":"Linear regression analysis","author":"Yeswanth Chitirala","date":"2023-12-14","categories":["news","code","analysis"],"image":"image.jpg"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}